# java_spark_wordFreq_log_analysis

There are 4 tasks using Java in Spark:

1: For the plot_summaries.txt, get the top 10 most frequently used words in descending order. 

Log File Analysis
2: Calculating the average time between two commands. 

3: Counting the number of commands in every 15 minutes interval.

4: Counting and ordering the frequency of all distinct commands in descending order. If a command is an eclipse command, we need to count the frequency for each distinct type(CommandID) of eclipse commands.

The event log (log.xml in the project folder) contains all the editing/debugging events happened inside an IDE. By analysing this log, it can give insights about a programmer's coding manner.
